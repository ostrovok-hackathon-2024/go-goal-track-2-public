{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install scikit-learn\n",
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfL7wkIpvoEi",
        "outputId": "6de9ff05-6837-40b6-d4db-e7bb7352aa80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 colorlog-6.8.2 optuna-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "import optuna\n",
        "import os\n",
        "import json\n",
        "import psutil\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "tgvWAGdVvnMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "hKKHaovGufMq",
        "outputId": "61e50494-a8a0-4a8e-8359-f76f57e7e826"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type int64 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7bde46d31822>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfidf_data.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TF-IDF data exported to tfidf_data.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('rates_clean.csv')\n",
        "df = df.fillna('undefined')\n",
        "\n",
        "categories = ['class', 'quality', 'bathroom', 'bedding', 'capacity', 'club', 'bedrooms', 'balcony', 'view', 'floor']\n",
        "\n",
        "for cat in categories:\n",
        "    df[cat] = df[cat].astype(str)\n",
        "\n",
        "df['rate_name'] = df['rate_name'].astype(str)\n",
        "\n",
        "X = df['rate_name']\n",
        "y = df[categories]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Improved TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb',  # Character n-grams, including word boundaries\n",
        "    ngram_range=(1, 3),  # Unigrams, bigrams, and trigrams\n",
        "    max_features=10000,  # Increased to capture more features\n",
        "    min_df=2,  # Ignore terms that appear in less than 2 documents\n",
        "    max_df=0.95,  # Ignore terms that appear in more than 95% of the documents\n",
        "    sublinear_tf=True,  # Apply sublinear tf scaling\n",
        "    lowercase=True,  # Convert all characters to lowercase\n",
        "    strip_accents='unicode',  # Remove accents\n",
        "    norm='l2',  # L2 normalization of the vectors\n",
        "    use_idf=True,  # Enable inverse-document-frequency reweighting\n",
        "    smooth_idf=True,  # Smooth idf weights by adding one to document frequencies\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "vocabulary = tfidf.vocabulary_\n",
        "doc_freq = tfidf.idf_.tolist()\n",
        "num_docs = tfidf.idf_.shape[0]\n",
        "doc_freq_dict = {term: int(tfidf.idf_[idx]) for term, idx in vocabulary.items()}\n",
        "\n",
        "tfidf_data = {\n",
        "    \"vocabulary\": vocabulary,\n",
        "    \"idf_values\": {term: tfidf.idf_[idx] for term, idx in vocabulary.items()},\n",
        "    \"doc_freq\": doc_freq_dict,\n",
        "    \"num_docs\": num_docs\n",
        "}\n",
        "\n",
        "with open(\"tfidf_data.json\", \"w\") as f:\n",
        "    json.dump(tfidf_data, f, indent=2)\n",
        "\n",
        "print(\"TF-IDF data exported to tfidf_data.json\")\n",
        "\n",
        "label_encoders = {}\n",
        "for category in categories:\n",
        "    le = LabelEncoder()\n",
        "    y_train[category] = le.fit_transform(y_train[category])\n",
        "    y_test[category] = le.transform(y_test[category])\n",
        "    label_encoders[category] = le\n",
        "\n",
        "labels_dir = \"labels\"\n",
        "os.makedirs(labels_dir, exist_ok=True)\n",
        "\n",
        "for category in categories:\n",
        "    with open(os.path.join(labels_dir, f\"labels_{category}.json\"), \"w\") as f:\n",
        "        json.dump(label_encoders[category].classes_.tolist(), f)\n",
        "\n",
        "print(\"Labels exported\")\n",
        "\n",
        "def objective(trial, X_train, y_train, X_test, y_test, category):\n",
        "    params = {\n",
        "        'iterations': trial.suggest_int('iterations', 100, 500),\n",
        "        'depth': trial.suggest_int('depth', 1, 5),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
        "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
        "        'verbose': 0,\n",
        "        'task_type': 'CPU',\n",
        "        'thread_count': psutil.cpu_count(logical=False),\n",
        "        'used_ram_limit': f'{int(psutil.virtual_memory().available / (1024 * 1024 * 1024) * 0.8)}GB',\n",
        "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
        "        'leaf_estimation_method': trial.suggest_categorical('leaf_estimation_method', ['Newton', 'Gradient']),\n",
        "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
        "        'subsample': trial.suggest_float('subsample', 0.1, 1.0) if trial.params['bootstrap_type'] != 'Bayesian' else None\n",
        "    }\n",
        "\n",
        "    model = CatBoostClassifier(**params)\n",
        "    model.fit(X_train, y_train[category], eval_set=(X_test, y_test[category]), early_stopping_rounds=50, verbose=0)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1_score = classification_report(y_test[category], y_pred, output_dict=True)['weighted avg']['f1-score']\n",
        "\n",
        "    return f1_score\n",
        "\n",
        "# Training and evaluation\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "models_dir = f\"models_{timestamp}\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "models = {}\n",
        "for category in categories:\n",
        "    print(f\"Training model for {category}\")\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(lambda trial: objective(trial, X_train_tfidf, y_train, X_test_tfidf, y_test, category), n_trials=100)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_params['verbose'] = 100\n",
        "    best_params['task_type'] = 'CPU'\n",
        "    model = CatBoostClassifier(**best_params)\n",
        "\n",
        "    model.fit(X_train_tfidf, y_train[category])\n",
        "\n",
        "    model_path = os.path.join(models_dir, f\"catboost_model_{category}.cbm\")\n",
        "    model.save_model(model_path)\n",
        "\n",
        "    models[category] = model\n",
        "\n",
        "    with open(os.path.join(models_dir, f\"best_parameters_{category}.json\"), \"w\") as f:\n",
        "        json.dump(best_params, f, indent=2)\n",
        "\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    classification_rep = classification_report(y_test[category], y_pred)\n",
        "\n",
        "    with open(os.path.join(models_dir, f\"classification_report_{category}.txt\"), \"w\") as f:\n",
        "        f.write(classification_rep)\n",
        "\n",
        "print(f\"Training completed. Models and results saved in {models_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction function\n",
        "def predict(rate_name):\n",
        "    input_data = pd.Series([rate_name])\n",
        "    input_tfidf = tfidf.transform(input_data)\n",
        "    result = {}\n",
        "\n",
        "    for category in categories:\n",
        "        prediction = models[category].predict(input_tfidf)[0]\n",
        "        result[category] = label_encoders[category].inverse_transform([prediction])[0]\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example predictions\n",
        "example1 = \"deluxe triple room\"\n",
        "example2 = \"Premium Two Queen Room with Living Area High Floor non-smoking\"\n",
        "\n",
        "print(json.dumps(predict(example1), indent=2))\n",
        "print(json.dumps(predict(example2), indent=2))"
      ],
      "metadata": {
        "id": "swX1iBNFvw3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load the saved CatBoost model\n",
        "model = CatBoostClassifier()\n",
        "model.load_model('catboost_model_quality.cbm')\n",
        "\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
        "\n",
        "def preprocess_input(text):\n",
        "    # Implement any text cleaning or preprocessing here\n",
        "    # For example: lowercase, remove punctuation, etc.\n",
        "    # return text.lower()\n",
        "    return text\n",
        "\n",
        "def vectorize_input(text):\n",
        "    # Transform the input text using the loaded TF-IDF vectorizer\n",
        "    return tfidf_vectorizer.transform([text])\n",
        "\n",
        "def predict(text):\n",
        "    # Preprocess the input text\n",
        "    preprocessed_text = preprocess_input(text)\n",
        "\n",
        "    # Vectorize the preprocessed text\n",
        "    vectorized_text = vectorize_input(preprocessed_text)\n",
        "    print(vectorized_text)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(vectorized_text)\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Example usage\n",
        "input_text = \"King Premium Mountain View no balcony\"\n",
        "result = predict(input_text)\n",
        "print(f\"Prediction: {result}\")\n",
        "\n",
        "# If your model returns probability scores, you can use predict_proba instead\n",
        "# probability = model.predict_proba(vectorized_text)\n",
        "# print(f\"Prediction probabilities: {probability}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuJkmMjDrfkx",
        "outputId": "f18f84df-48cc-4b05-fc46-8348bdcdcad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 133)\t0.05159103942290583\n",
            "  (0, 134)\t0.11686431517097241\n",
            "  (0, 219)\t0.08611483890237456\n",
            "  (0, 223)\t0.08701424709009065\n",
            "  (0, 236)\t0.13769439337993772\n",
            "  (0, 241)\t0.18478591666814925\n",
            "  (0, 245)\t0.12032907478739258\n",
            "  (0, 250)\t0.12241903445421606\n",
            "  (0, 264)\t0.08896645384864628\n",
            "  (0, 271)\t0.11488159657231892\n",
            "  (0, 319)\t0.0751312934772496\n",
            "  (0, 322)\t0.07541877184237387\n",
            "  (0, 857)\t0.07376255595637449\n",
            "  (0, 916)\t0.1391520143352493\n",
            "  (0, 921)\t0.1634984753876855\n",
            "  (0, 928)\t0.09723173060848084\n",
            "  (0, 932)\t0.12963105107316736\n",
            "  (0, 1043)\t0.043513119802300396\n",
            "  (0, 1054)\t0.11254126515953183\n",
            "  (0, 1058)\t0.12969669888726051\n",
            "  (0, 1115)\t0.0614275419321618\n",
            "  (0, 1178)\t0.09924562953013805\n",
            "  (0, 1186)\t0.11373091457124099\n",
            "  (0, 1450)\t0.12112889668753928\n",
            "  (0, 1453)\t0.12476419997129916\n",
            "  :\t:\n",
            "  (0, 2591)\t0.07887086645582893\n",
            "  (0, 2609)\t0.12989443459511288\n",
            "  (0, 2663)\t0.06636099323255523\n",
            "  (0, 2669)\t0.14286117005301907\n",
            "  (0, 2692)\t0.06279961347242877\n",
            "  (0, 2760)\t0.11321009245762832\n",
            "  (0, 2761)\t0.11926023410713625\n",
            "  (0, 2786)\t0.036354862843811286\n",
            "  (0, 2842)\t0.08412382284970739\n",
            "  (0, 2853)\t0.12453883089043977\n",
            "  (0, 3133)\t0.039781364463571216\n",
            "  (0, 3158)\t0.0930213861664691\n",
            "  (0, 3163)\t0.2026116500193632\n",
            "  (0, 3274)\t0.06725542359777663\n",
            "  (0, 3321)\t0.1317298544687775\n",
            "  (0, 3322)\t0.13367413178588122\n",
            "  (0, 3328)\t0.10690201183902397\n",
            "  (0, 3337)\t0.16719991023876157\n",
            "  (0, 3383)\t0.06811170028883846\n",
            "  (0, 3409)\t0.07417471718286063\n",
            "  (0, 3412)\t0.07788170171863412\n",
            "  (0, 3426)\t0.05251444581793041\n",
            "  (0, 3427)\t0.0809434800256566\n",
            "  (0, 3531)\t0.07813984698039138\n",
            "  (0, 3532)\t0.08276100577763608\n",
            "Prediction: [[10]]\n"
          ]
        }
      ]
    }
  ]
}